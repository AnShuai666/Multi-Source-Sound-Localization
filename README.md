# Multi-Source-Sound-Localization

This is a PyTorch implementation that aims to perform sound localization in complex audiovisual scenes, where there multiple objects making sounds. We disentangle a complex scene into several simple scenes consisting of one-to-one sound-object pairs.

We propose a two-stage learning framework, which establishes coarse-grained audiovisual correspondence in the category level at the first stage, and achieves fine-grained sound-object alignment at the second stage.

## Prepare Dataset

#### SoundNet-Flickr Dataset

The audiovisual pairs are defined as one frame and a corresponding 5-second audio clip. We resize the image into $256\times 256$, sample the audio at $22050$ Hz, and convert it into log-mel spectrogram. We then convert the image and audio into hdf5 file format. We use the first-level labels in AudioSet for classification, and use pretrained CRNN and ResNet-18 to generate pseudo labels. The mapping from predictions to these 7 categories are stored in cluster_a.npy and cluster_v3.npy.

#### AVE Dataset

There are totally 4143 10-second video clips available. We extract video frames at $1$ fps, and resize the images into $256\times 256$, sample the audio at $22050Hz$, and convert it into log-mel spectrogram. We then convert the image and audio into hdf5 file format. We use the first-level labels in AudioSet for classification, and use pretrained CRNN and ResNet-18 to generate pseudo labels. The mapping from predictions to these 7 categories are stored in cluster_a.npy and cluster_v3.npy.

#### AudioSet Instrument Dataset

This is a subset of AudioSet covering 15 music instruments. The video clips are annotated with labels that indicate which music instruments make sound in the audio.  We extract video frames at $1$ fps, and resize the images into $256\times 256$, sample the audio at $22050Hz$, and convert it into log-mel spectrogram. The video index and tags for training and testing are stored in train_audio.pkl and val_audio.pkl. And the bounding box annotations generated by Faster RCNN for evaluation are stored in normav.json in COCO format.

For unlabeled videos in SoundNet-Flickr and AVE dataset, it is optional to introduce class-agnostic proposals generated by Faster RCNN, and perform classification on each ROI region to improve the quality of pseudo labels.

## Training

#### Training 1st stage

For SoundNet-Flickr or AVE dataset, run 

```
./train_avc.sh
optional argumets:
[--train-batch] training batchsize
[--val-batch] validation batchsize
[--dataset] the name of dataset
[--mix] the number videos mixed per batch
[--frame] the number of frames in an audiovisual pair
[--lr] starting learning rate
[--schedule] epoch at which learning rate decay
[--resume] load checkpoint and continue training
```

For AudioSet dataset, run

```
./train_audioset_avc.sh
[--train-batch] training batchsize
[--val-batch] validation batchsize
[--dataset] the name of dataset
[--mix] the number videos mixed per batch
[--frame] the number of frames in an audiovisual pair
[--lr] starting learning rate
[--schedule] epoch at which learning rate decay
[--resume] load checkpoint and continue training
```

#### Training 2nd stage

For SoundNet-Flickr or AVE dataset, run 

```
./train_joint.sh
optional argumets:
[--train-batch] training batchsize
[--val-batch] validation batchsize
[--dataset] the name of dataset
[--mix] the number videos mixed per batch
[--frame] the number of frames in an audiovisual pair
[--lr] starting learning rate
[--schedule] epoch at which learning rate decay
[--resume] load checkpoint and continue training
```

For AudioSet dataset, run

```
./train_joint.sh
optional argumets:
[--train-batch] training batchsize
[--val-batch] validation batchsize
[--dataset] the name of dataset
[--mix] the number videos mixed per batch
[--frame] the number of frames in an audiovisual pair
[--lr] starting learning rate
[--schedule] epoch at which learning rate decay
[--resume] load checkpoint and continue training
```

The training log file and trained model are stored in

```
checkpoint/datasetName/log.txt
checkpoint/datasetName/model_avc.pth.tar
checkpoint/datasetName/model_joint.pth.tar
```

## Evaluate

For quantitative evaluation on human annotated SoundNet-Flickr, run

```
./eval.sh
```

It outputs cIoU and AUC result, and the visualization of localization maps.

For evaluation on AudioSet Instrument dataset, run

```
./eval_audioset.sh
```

It outputs class-specific localization maps on each sample stored in infer.npy, then run

```
python3 utils/evaluate.py
```

to calculate evaluation results and visualize localization maps on different difficulty levels.